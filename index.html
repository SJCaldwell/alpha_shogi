<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Alpha Zero</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
					<section data-markdown>
							<textarea data-template>
							### Alpha ~~Shogi~~ Connect 4

							Shane & Noah
							</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								### History of Chess Programs

								- Long been a focus of artificial intelligence research
								- Alan Turing developed one in 1956
								- Two years before "Artificial Intelligence" existed
								</textarea>
							</section>
							<section data-markdown>
								<textarea data-template>
								### Why? Branching factor makes normal evaluation intractable
								![](image/branching_factor.jpg)
								</textarea>
							</section>
							
							<section data-markdown>
								<textarea data-template>
								### 62 years later

								- Relied on data of human-played games
								- Human coded features
								- Collaboration wih grandmasters
								- Studying openings
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								### AlphaGo
								- Combine deep learning and reinforcement learning 
								- Start by learning famous openings/endgame until the neural network learns them.
								- Go is translationally invariant, meaning is the same wherever something is placed. 

								![](image/translational_invariance.png)

								</textarea>
						</section>
						<section data-markdown>
							<textarea data-template>
							### AlphaGo

							- Points are defined in terms of liberties, (which are largely local)
							- No draws in Go, only binary wins and losses (simpler loss function)

							Training wheel approach
							1. Bootstrap network with supervised learning on "real" games
							2. Once developed, extend abilities with reinforcement learning
							</textarea>
					</section>
						<section data-markdown>
							<textarea data-template>
							### Alpha Zero: Removing the training wheels
							- No human games included to do supervised learning on
							- Only raw board added as input, not human-curated features
							- Policy network and value network combined into a single residual network
							- *Only* uses reinforcement learning, starting from playing random moves. 
							</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
						### How AlphaZero works with Connect 4

						(Assuming Batch Size of 1 for simplicity)
						</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
					### Input

					- N x N x (MT + L) image stack
					- N is board size
					- M is the number of planes needed for pieces
					- T is the step-history, set to 8 by default
					- L is a number of constant valued input planes, denoting character color, total move count, and special rules "for chess, this would be whether castling weas an option, or whether a draw was coming"
					</textarea>
			</section>
			<section data-markdown>
				<textarea data-template>
				### Connect 4 input

				- 6 x 7 x (1 * 8  + 1) image stack
				- N is board size
				- M is the number of planes needed for pieces
				- T is the step-history, set to 8 by default
				- L is a number of constant valued input planes.

				Only one piece type allows us to use one input plane, with -1 and 1 to determine color
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### Network
				- Really the only place the algorithm loses generality
				- Varies from game to game
				- For a simple games with small boards and one piece type, something like four convolutional layers and two fully connected layers is almost overkill
				- For complex games like chess, 20 residual layers with 2 convolutional layers a piece is not uncommon
				</textarea>
		</section>
		<section data-markdown>
			<textarea data-template>
			### Output (Policy Vector)

			- Softmax used at final layer, each neuron representing one action
			- For chess, there are over 6,000 of these 
			- For Connect 4, there are 7
			![](image/neural_net_layer.png)

			</textarea>
	</section>
	<section data-markdown>
		<textarea data-template>
		### Output (Policy Vector)

	Probability is assigned to each option, with the highest probability being the chosen neuron 

		![](image/fired_neuron.png)
		</textarea>
</section>
	<section data-markdown>
		<textarea data-template>
		### Value Network
		- Predicts the winner of the game from any board state 
		- Uses the same convolutional output to understand abstract board state features
		- Smushes that down through a final layer to get a single scalar value [-1, 1]

		- 1 certain win 
		- -1 certain loss
		</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	### Loss Functions
- Minimize total loss by adding policy vector loss and value network loss
- This creates one total loss function
![](image/loss_function.png)
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>

	- The branching factor makes traditional search methods choke
	- How does AlphaZero make search *tractable*?
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
### Reduce Breadth
- Policy Network reduces breadth
- If search only considers moves based on the policy network, there's much less trees to expand and evaluate
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
### Reduce Depth
- Value network takes in boardstate and outputs probability of victory
- At any point, we can use the value network to replace any search tree value with one number.
- So we can truncate any leaf of the tree without rolling it out to completion
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
### Monto-Carlo Tree Search 
- Uses current neural network to guide tree search 
- Compress everything achieved by tree search into neural network 
- Value network is trained to predict the winner of the game played by self play (based on each position of the game)
	</textarea>
</section>
<section data-markdown>
		<textarea data-template>
###  Network Structure
- The network is what varies from implementation to implementation 

- We only want to train a singular final model to battle Gary
		</textarea>
</section>
<section data-markdown>
		<textarea data-template>
###  Game plan
- Generate 6 different models, allowing them all to train for the same amount of games
- Pit them against each other in a round-robin tournament
- The winner trains for multiple days to 1v1 Gary
		</textarea>
</section>
						<section data-markdown>
								<textarea data-template>
						### Early failures
						- A trained model is only ever saved if it can defeat an opponent neural net initialized with random weights
						- We lost two in infancy: 
						- 1. We tried a 5x5 kernel for convolutional layers, which seemed too large for the 6x7 board "images"
						- 2. Logit function instead of relu also died young
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 1
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 2
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 3
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 4
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Training 
						- One starts with a randomly initialized set of neural network parameters
						- In each of these games, for each move, Monte Carlo tree search is used to calculate Ï€. The final outcome of each game determines that game's value for Z
						- Minimize loss in parameters through gradient descent
								</textarea>
						</section>

						<section data-markdown>
								<textarea data-template>
								Show where we are compared to how did
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
									Display results here textually
									include graphic
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								### Future Work

								- Better hardware
								- More training time 
								- A championship?
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								ONE MORE TIME

								</textarea>
						</section>
						<section data-markdown>
							<textarea data-template>
							![](image/Alpha4_v_Gary.jpg)

							</textarea>
					</section>
					<section data-markdown>
							<textarea data-template>
							### Questions?
							</textarea>
					</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
