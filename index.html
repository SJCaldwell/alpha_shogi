<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Alpha Connect4</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
					<section data-markdown>
							<textarea data-template>
							### Alpha ~~Shogi~~ Connect 4

							Shane & Noah
							</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								### History of Chess Programs

								- Long been a focus of artificial intelligence research
								- Alan Turing developed one in 1956
								- Two years before "Artificial Intelligence" existed
								</textarea>
							</section>
							<section data-markdown>
								<textarea data-template>
								### Why? Branching factor makes normal evaluation intractable
								![](image/branching_factor.jpg)
								</textarea>
							</section>
							
							<section data-markdown>
								<textarea data-template>
								### 62 years later

								- Relied on data of human-played games
								- Human coded features
								- Collaboration wih grandmasters
								- Studying openings
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								### AlphaGo
								- Combine deep learning and reinforcement learning 
								- Start by learning famous openings/endgame until the neural network learns them.
								- Go is translationally invariant, meaning is the same wherever something is placed. 

								![](image/translational_invariance.png)

								</textarea>
						</section>
						<section data-markdown>
							<textarea data-template>
							### AlphaGo

							- Points are defined in terms of liberties, (which are largely local)
							- No draws in Go, only binary wins and losses (simpler loss function)

							Training wheel approach
							1. Bootstrap network with supervised learning on "real" games
							2. Once developed, extend abilities with reinforcement learning
							</textarea>
					</section>
						<section data-markdown>
							<textarea data-template>
							### Alpha Zero: Removing the training wheels
							- No human games included to do supervised learning on
							- Only raw board added as input, not human-curated features
							- Policy network and value network combined into a single residual network
							- *Only* uses reinforcement learning, starting from playing random moves. 
							</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
						### How AlphaZero works with Connect 4

						(Assuming Batch Size of 1 for simplicity)
						</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
					### Input

					- (MT + L) stack of NxN images
					- N is board size
					- M is the number of planes needed for pieces
					- T is the step-history, set to 8 by default
					- L is a number of constant valued input planes, denoting character color, total move count, and special rules (for chess, this would be whether castling was an option, or whether a draw was coming)
					</textarea>
			</section>
			<section data-markdown>
				<textarea data-template>
				### Connect 4 input

				- (1 * 1  + 0) stack of 6x7 images

				Only one piece type allows us to use one input plane
                                - 1 and -1 to determine color
                                - 0 to indicate empty square
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### Network
				- Really the only place the algorithm loses generality
				- Varies from game to game
				- For a simple games with small boards and one piece type, something like four convolutional layers and two fully connected layers is almost overkill
				- For complex games like chess, 20 residual layers with 2 convolutional layers a piece is not uncommon
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### First Convolutional Layer
				- 
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### Second Convolutional Layer
				- 
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### Third Convolutional Layer
				- 
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### Fourth Convolutional Layer
				- 
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### Two Fully Connected Layers
				- 
				</textarea>
		</section>
		<section data-markdown>
			<textarea data-template>
			### Output (Policy Vector)

			- Softmax used at final layer, each neuron representing one action
			- For chess, there are over 6,000 of these 
			- For Connect 4, there are 7
			![](image/neural_net_layer.png)

			</textarea>
	</section>
	<section data-markdown>
		<textarea data-template>
		### Output (Policy Vector)

	Probability is assigned to each option, with the highest probability being the chosen neuron 

		![](image/fired_neuron.png)
		</textarea>
</section>
<section data-markdown>
		<textarea data-template>
		### Rules are defined for logical masking

		- Neural network doesn't know anything specific about the rules 
		- Means it can recommend illegal moves
		</textarea>
</section>
<section data-markdown>
		<textarea data-template>
		![](image/full_column.png)

		</textarea>
</section>
<section data-markdown>
		<textarea data-template>
		![](image/illegal_man.png)

		- The illegal value is forced to 0, and the the probabilities are redistributed elsewhere
		</textarea>
</section>
	<section data-markdown>
		<textarea data-template>
		### Output (Value Network)
		- Predicts the winner of the game from any board state 
		- Uses the same convolutional output to understand abstract board state features
		- Smushes that down through a final layer to get a single scalar value [-1, 1]

		- 1 certain win 
		- -1 certain loss
		</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	### Loss Functions
- Get total loss by adding policy vector loss and value network loss
- v(s) is the network's value output for board state s
- p(s) is the network's policy vector output
- z is the eventual result (win/loss)
- pi is the improved stochastic policy vector from MCTS 
![](image/loss_function.png)
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	### Loss Functions
- Training examples are summed over a batch of examples
- This batch is randomly selected from past data
![](image/loss_function.png)
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	### Generating Examples
- Where does this data come from?
- Self-play games
- At every turn, save (s, pi, z)
- z is filled in after the game concludes
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	### Batch Size
- Previously we described the network with a batch size of 1
- To compute sums efficiently the actual network is extended to handle arbitrary batch sizes
- All inputs and outputs through every layer have a batch_size dimension added
- Batch size policy vectors and values
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	### Training
- Adam Optimizer acts on batches
- Extension of Stochastic Gradient Descent
- Separate learning rates for every network parameter
![](image/loss_function.png)

	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	- The branching factor makes traditional search methods choke
	- How does AlphaZero make search *tractable*?
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
### Reduce Breadth
- Policy Network reduces breadth
- If search only considers moves based on the policy network, there's much less trees to expand and evaluate
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
### Reduce Depth
- Value network takes in boardstate and outputs probability of victory
- At any point, we can use the value network to replace any search tree value with one number.
- So we can truncate any leaf of the tree without rolling it out to completion
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
### Monto-Carlo Tree Search 
- Uses current neural network to guide tree search 
- Compress everything achieved by tree search into neural network 
- Value network is trained to predict the winner of the game played by self play (based on each position of the game)
	</textarea>
</section>
<section data-markdown>
		<textarea data-template>
###  Choosing a Network Structure
- The network is what varies from implementation to implementation 
- Generate 6 different models, allowing them all to train for the same amount of games
- Pit them against each other in a tournament
- The winner trains for multiple days
		</textarea>
</section>
						<section data-markdown>
								<textarea data-template>
						### Early failures
						- A trained model is only ever saved if it can defeat an opponent neural net initialized with random weights
						- We lost two in infancy: 
						- 1. We tried a 5x5 kernel for convolutional layers, which seemed too large for the 6x7 board "images"
						- 2. Logit function instead of relu also died young
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 1
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 2
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 3
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 4
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Training 
						- The winner was stock 
						- It's trained for about two days 
						- Or [X NUMBER ] of matches
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								ONE MORE TIME

								</textarea>
						</section>
						<section data-markdown>
							<textarea data-template>
							![](image/Alpha4_v_Gary.jpg)

							</textarea>
					</section>

						<section data-markdown>
								<textarea data-template>
								Progress on Shogi side of things 
								- Game logic is being coded up
								- Network architecture is well in progress, something large like 20 residual layers with 2 convolutional layers a piece. 
								- Probably need better hardware
								- (Were being told we're RAM hogs as is)
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								- Input size blows up - huge number of different kinds of pieces and special rules 
								- Coding those up by hand is busy work in a way
								- Switched to get quicker training turnaround 
								- And a game people actually knew
								</textarea>
						</section>
					<section data-markdown>
							<textarea data-template>
							### Questions?
							</textarea>
					</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
