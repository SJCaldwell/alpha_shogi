<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Alpha Connect4</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
					<section data-markdown>
							<textarea data-template>
							### Alpha ~~Shogi~~ Connect 4

							Shane & Noah
							</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								### History of Chess Programs

								- Long been a focus of artificial intelligence research
								- Alan Turing developed one in 1956
								- Two years before "Artificial Intelligence" existed
								</textarea>
							</section>
							<section data-markdown>
								<textarea data-template>
								### Why? Branching factor makes normal evaluation intractable
								![](image/branching_factor.jpg)
								</textarea>
							</section>
							
							<section data-markdown>
								<textarea data-template>
								### 62 years later

								- Relied on data of human-played games
								- Human coded features
								- Collaboration wih grandmasters
								- Studying openings
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								### AlphaGo
								- Combine deep learning and reinforcement learning 
								- Start by learning famous openings/endgame until the neural network learns them.
								- Go is translationally invariant, meaning is the same wherever something is placed. 

								![](image/translational_invariance.png)

								</textarea>
						</section>
						<section data-markdown>
							<textarea data-template>
							### AlphaGo

							- Points are defined in terms of liberties, (which are largely local)
							- No draws in Go, only binary wins and losses (simpler loss function)

							Training wheel approach
							1. Bootstrap network with supervised learning on "real" games
							2. Once developed, extend abilities with reinforcement learning
							</textarea>
					</section>
						<section data-markdown>
							<textarea data-template>
							### Alpha Zero: Removing the training wheels
							- No human games included to do supervised learning on
							- Only raw board added as input, not human-curated features
							- Policy network and value network combined into a single residual network
							- *Only* uses reinforcement learning, starting from playing random moves. 
							</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
						### How AlphaZero works with Connect 4

						(Assuming Batch Size of 1 for simplicity)
						</textarea>
				</section>
				<section data-markdown>
					<textarea data-template>
					### Input

					- (MT + L) stack of NxN images
					- N is board size
					- M is the number of planes needed for pieces
					- T is the step-history, set to 8 by default
					- L is a number of constant valued input planes, denoting character color, total move count, and special rules (for chess, this would be whether castling was an option, or whether a draw was coming)
					</textarea>
			</section>
			<section data-markdown>
				<textarea data-template>
				### Connect 4 input

				- (1 * 1  + 0) stack of 6x7 images

				Only one piece type allows us to use one input plane

				- 1 and -1 to determine color
				- 0 to indicate empty square
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### Hidden Layer Architecture
				- Really the only place the algorithm loses generality
				- Varies from game to game
				- For a simple games with small boards and one piece type, something like four convolutional layers and two fully connected layers is almost overkill
				- For complex games like chess, 20 residual layers with 2 convolutional layers a piece is a minimum
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### First Convolutional Layer

				- 3x3 convolutional kernel
				- One 6 by 7 input feature plane
				- 512 different kernels and convolutions produce 512 output planes of the same size
				- ReLu activation function
				- Batch Normalization
				- 9 * 512 = 4608 weights
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### Second Convolutional Layer
				- Same as the first
				- Each of the 512 input planes has a single convolution applied to it, producing 512 output planes of the same size
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### Third Convolutional Layer
				- Convolution without padding
				- 3x3 kernel cuts off 1 unit margins
				- 512 output planes of size 4 x 5
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### Fourth Convolutional Layer
				- Again a convolution without padding
				- 512 output planes of size 2 x 3
				</textarea>
		</section>
			<section data-markdown>
				<textarea data-template>
				### Two Fully Connected Layers
				- The 4th convolutional layer planes are flattened and stacked together
				- Two fully connected layers attach to that, bringing the size down first to 1024 and then to 512 (again arbitrary numbers)
				- ReLu activation functions, with batch norm and dropout
				- This is the final hidden layer output
				</textarea>
		</section>
		<section data-markdown>
			<textarea data-template>
			### Output (Policy Vector)
			- One of two output heads
			- A fully connected layer connects to the last hidden layer
			- Softmax used to create probability distribution over neurons
			- Each neuron represents one action
			- For chess, there are over 6,000 of these 
			- For Connect 4, there are 7 (one per column)
			![](image/neural_net_layer.png)

			</textarea>
	</section>
	<section data-markdown>
		<textarea data-template>
		### Output (Policy Vector)

		Probability is assigned to each option, with the highest probability being the chosen neuron 

		![](image/fired_neuron.png)
		</textarea>
</section>
<section data-markdown>
		<textarea data-template>
		### Rules are defined for logical masking

		- Neural network doesn't know anything specific about the rules 
		- Means it can recommend illegal moves
		</textarea>
</section>
<section data-markdown>
		<textarea data-template>
		![](image/full_column.png)

		</textarea>
</section>
<section data-markdown>
		<textarea data-template>
		![](image/illegal_man.png)

		- The illegal value is forced to 0, and the the probabilities are renormalized
		</textarea>
</section>
	<section data-markdown>
		<textarea data-template>
		### Output (Value Network)
		- A separate output head which connects to the last hiddden layer
		- Predicts the winner of the game from any board state 
		- A simple fully connected layer with a hyperbolic tangent activation function
		- Tanh smushes the output into a single scalar value [-1, 1]

		- 1 certain win 
		- -1 certain loss
		</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	### Loss Functions
- Get total loss by adding policy vector loss and value network loss
- v(s) is the network's value output for board state s
- p(s) is the network's policy vector output
- z is the eventual result (win/loss)
- pi is the improved stochastic policy vector from MCTS 
![](image/loss_function.png)
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	### Loss Functions
- Training examples are summed over a batch of examples
- This batch is randomly selected from past data
![](image/loss_function.png)
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	### Generating Examples
- Where does this data come from?
- Self-play games
- At every turn, save (s, pi, z)
- z is filled in after the game concludes
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	### Batch Size
- Previously we described the network with a batch size of 1
- To compute sums efficiently the actual network is extended to handle arbitrary batch sizes
- All inputs and outputs through every layer have a batch_size dimension added
- Batch size policy vectors and values
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	### Training
- Adam Optimizer acts on batches
- Extension of Stochastic Gradient Descent
- Separate learning rates for every network parameter
![](image/loss_function.png)
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
	  ### What's the point?
	- The branching factor makes traditional search methods choke
	- How does AlphaZero make search *tractable*?
	- How do the policy vector and value vector help?
	
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
### Reduce Breadth
- Policy Network reduces breadth
- If search only considers moves based on the policy network, there's much less trees to expand and evaluate
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
### Reduce Depth
- Value network takes in board state and outputs probability of victory
- At any point, we can use the value network to replace any search tree value with one number.
- So we can truncate any leaf of the tree without rolling it out to completion
	</textarea>
</section>
<section data-markdown>
	<textarea data-template>
### Monto-Carlo Tree Search 
- Uses current neural network to guide tree search 
- Compress everything achieved by tree search into neural network 
- Value network is trained to predict the winner of the game played by self play (based on each position of the game)
	</textarea>
</section>
<section data-markdown>
		<textarea data-template>
###  Choosing a Network Structure
![](image/competition.gif)
		</textarea>
</section>
<section data-markdown>
		<textarea data-template>
###  Choosing a Network Structure
- The network is what varies from implementation to implementation 
- Generate 6 different models, allowing them all to train for the same amount of games
- Pit them against each other in a tournament
- The winner trains for multiple days
		</textarea>
</section>
						<section data-markdown>
								<textarea data-template>
						### Early failures
						- A trained model is only ever saved if it can defeat an opponent neural net initialized with random weights
						- We lost two in infancy: 
						1. We tried a 5x5 kernel for convolutional layers, which seemed too large for the 6x7 board "images"
						2. Logit function instead of relu also died young
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 1
						- The network already described, with 4 convolutional layers and 2 fully connected
						- "Stock"
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 2
						- Leaky ReLu activation functions for all hidden layers
						- Leaky ReLu has a small positive gradient when x &lt; 0 (e.g. 0.01*x)
						- "Leaky"
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 3
						- Removed dropout from last two hidden dense layers
						- "No Dropout"
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Fighter 4
						- Two additional hidden convolutional layers, with padding
						- "Big Boy"
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Colosseum
						- All 4 fighters trained for 1 iteration (100 self-play games)
						- Each fighter then played 50 matches against every other
						- In the end no fighter won more than 60% of games against any other
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
						### Training
						- Stock performed marginally better
						- It's been trained for about two days 
						- Or 1300 matches
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								ONE MORE TIME

								</textarea>
						</section>
						<section data-markdown>
							<textarea data-template>
							![](image/Alpha4_v_Gary.jpg)

							</textarea>
					</section>

						<section data-markdown>
								<textarea data-template>
								Progress on Shogi side of things 
								- Game logic is still being coded up
								- Network architecture is well in progress, something large like 20 residual layers with 2 convolutional layers a piece. 
								- Probably need better hardware
								- (Were being told we're RAM hogs as is)
								</textarea>
						</section>
						<section data-markdown>
								<textarea data-template>
								- Input size blows up - huge number of different kinds of pieces and special rules 
								- Coding those up by hand is busy work in a way
								- Switched to get quicker training turnaround 
								</textarea>
						</section>
					<section data-markdown>
							<textarea data-template>
							### Questions?
							</textarea>
					</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
